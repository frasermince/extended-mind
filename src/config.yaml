# the name of this experiment
exp_name: ${oc.env:EXP_NAME,${hydra:runtime.filename}}
agent_name: "main_dqn"
run_folder: "./runs"
parquet_folder: "./parquet_metrics"
# seed of the experiment
seed: 1
# if toggled, this experiment will be tracked with Weights and Biases
track: true
# if toggled (while above is toggled), this will be tracked but in offline mode
track_offline: false
# wandb api key
wandb_api_key: ""
# the wandb's project name
wandb_project_name: "SaltAndPepper"
# the entity (team) of wandb's project
wandb_entity: "frasermince"
# whether to capture videos of the agent performances (check out `videos` folder)
capture_video: false
# whether to save model into the `runs/{run_name}` folder
save_model: false
# whether to upload the saved model to huggingface
upload_model: false
# the user or org name of the model repository from the Hugging Face Hub
hf_entity: ""

experiment_description: "logging-test"


agent_view_size: 3

path_mode: "NONE" # NONE, SHORTEST_PATH, SLIGHTLY_SUBOPTIMAL_PATH, SUBOPTIMAL_PATH, MISLEADING_PATH, VISITED_CELLS, RANDOM_PATH

nonstationary_path_decay_pixels: 720
nonstationary_path_decay_chance: .25
nonstationary_path_inclusion_pixels: 16
nonstationary_visitations_before_path_appearance: 1
nonstationary_steps_before_path_visible: 0
nonstationary_only_optimal: false
nonstationary_max_path_count: 1
tile_size: 8


show_landmarks: false

render_options:
  show_grid_lines: false
  show_walls_pov: false
  show_optimal_path: true


# Algorithm specific arguments
training:
  # the id of the environment
  env_id: "MiniGrid-SaltAndPepper-v0-custom"
  # total timesteps of the experiments
  total_timesteps: 100000
  # the learning rate of the optimizer
  learning_rate: 1e-4
  # the number of parallel game environments
  num_envs: 1
  # the replay memory buffer size
  buffer_size: 1000000
  # the discount factor gamma
  gamma: 0.95
  # the target network update rate
  tau: 1.0
  # the timesteps it takes to update the target network
  target_network_frequency: 500
  # the batch size of sample from the replay memory
  batch_size: 16
  # the starting epsilon for exploration
  start_e: 1.0
  # the ending epsilon for exploration
  end_e: 0.01
  # the fraction of `total-timesteps` it takes from start-e to go end-e
  exploration_fraction: 0.10
  # timestep to start learning
  learning_starts: 5000
  # the frequency of training
  train_frequency: 1
  dense_features: [8, 8]

eval_params:
  action_mode: "RECORDED_ACTIONS" # "FINAL_POLICY", "RECORDED_ACTIONS", "HARDCODED"
  parquet_path: "./parquet_metrics/path_mode_NONE/learning_rate_0.0001/network_depth_3/network_width_8/seed_1/metrics/part-fb5807190ba341048b8a9c4f99ca10e2.parquet"
  hardcoded_actions: null

eval: false
train: true

dry_run: false

exp_group_id: "test_id"